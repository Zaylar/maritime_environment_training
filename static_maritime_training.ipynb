{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import wandb\n",
    "import math\n",
    "import winsound\n",
    "import random\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy\n",
    "from stable_baselines import PPO2, A2C, ACKTR, DDPG, SAC, TD3, TRPO\n",
    "\n",
    "from stable_baselines.ddpg.policies import MlpPolicy as ddpg_MlpPolicy, LnMlpPolicy as ddpg_LnMlpPolicy\n",
    "from stable_baselines.sac.policies import MlpPolicy as sac_MlpPolicy, LnMlpPolicy as sac_LnMlpPolicy\n",
    "from stable_baselines.td3.policies import MlpPolicy as td3_MlpPolicy, LnMlpPolicy as td3_LnMlpPolicy\n",
    "\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "root_dir = Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(env_kwargs, model_save_path, model_config_save_path, total_timesteps=200000, wandb_logger=None,\n",
    "                model_load_path=None, model_config_load_path=None, env_name='static_maritime:static-maritime-v0'\n",
    "               ):\n",
    "    if model_load_path:\n",
    "        with open(model_config_load_path) as f:\n",
    "            env_kwargs = json.load(f)\n",
    "        \n",
    "        env = DummyVecEnv([lambda: gym.make(env_name, env_kwargs=env_kwargs)])\n",
    "        model = PPO2.load(model_load_path, env)\n",
    "        print(\"model loaded\")\n",
    "    else:\n",
    "        env = DummyVecEnv([lambda: gym.make(env_name, env_kwargs=env_kwargs)])\n",
    "        model = PPO2(MlpPolicy, env, verbose=0, wandb_logger=wandb_logger)\n",
    "        print(\"model created\")\n",
    "        \n",
    "    model.learn(total_timesteps=total_timesteps)\n",
    "    print('Model has finished learning')\n",
    "    env.close()\n",
    "\n",
    "    model.save(model_save_path)\n",
    "    print(\"Model successfully saved\")\n",
    "\n",
    "    with open(model_config_save_path, 'w') as fp:\n",
    "        json.dump(env_kwargs, fp)\n",
    "\n",
    "    duration = 1000  # milliseconds\n",
    "    freq = 440  # Hz\n",
    "    winsound.Beep(freq, duration)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model_save_path, model_config_save_path, episodes=50, eval_wandb_project='test',\n",
    "               env_name='static_maritime:static-maritime-v0', max_timesteps=999\n",
    "              ):\n",
    "    with open(model_config_save_path) as f:\n",
    "        config = json.load(f)\n",
    "        \n",
    "    env = DummyVecEnv([lambda: gym.make(env_name, env_kwargs=config)])\n",
    "    \n",
    "    model = PPO2.load(model_save_path, env)\n",
    "    \n",
    "    if eval_wandb_project:\n",
    "        eval_wandb = wandb.init(project=eval_wandb_project, config=config)\n",
    "    \n",
    "    # Evaluate for n episodes\n",
    "    # Track cumulative reward\n",
    "    cumulative_reward = 0\n",
    "    \n",
    "    # How far the car was from the goal at each point. Smaller magnitude is better\n",
    "    total_position_difference = 0\n",
    "    goal_position = 0.45\n",
    "    for episode in range(episodes):\n",
    "        print(f'episode: {episode}')\n",
    "        obs = env.reset()\n",
    "        episode_reward = 0\n",
    "        \n",
    "        # The action count indicates the timestep\n",
    "        episode_action_count = 0\n",
    "        closest_position = -10\n",
    "        while True:\n",
    "            action, _states = model.predict(obs)\n",
    "            obs, rewards, done, info = env.step(action)\n",
    "            \n",
    "            if obs[0][0] > closest_position:\n",
    "                closest_position = obs[0][0]\n",
    "            episode_action_count += 1\n",
    "            episode_reward += rewards[0]\n",
    "\n",
    "            if eval_wandb_project:\n",
    "                # Timestep logs\n",
    "                eval_wandb.log({\"x_coordinate\": obs[0][0]})\n",
    "                eval_wandb.log({\"y_coordinate\": obs[0][1]})\n",
    "                eval_wandb.log({\"heading\": obs[0][2]})\n",
    "                eval_wandb.log({\"heading_delta\": action[0][0]})\n",
    "                eval_wandb.log({\"goal_distance\": info[0][\"goal_distance\"]})\n",
    "                \n",
    "            if done[0]:\n",
    "                print(f'episode action count: {episode_action_count}')\n",
    "                if episode_action_count == max_timesteps:\n",
    "                    print(f'Agent failed episode through timeout')\n",
    "                    \n",
    "                else:\n",
    "                    print(f'Agent completed episode under the timestep limit')\n",
    "                    \n",
    "                total_position_difference += abs(goal_position - closest_position)\n",
    "                    \n",
    "                break\n",
    "                \n",
    "        cumulative_reward += episode_reward\n",
    "\n",
    "        if eval_wandb_project:\n",
    "            # Episode logs\n",
    "            eval_wandb.log({\"episode\": episode, \"episode_reward\": episode_reward, \"cumulative_reward\": cumulative_reward})\n",
    "\n",
    "    # eval_wandb.finish()\n",
    "    print('evaluation complete')\n",
    "    avg_position_difference = total_position_difference / episodes\n",
    "    \n",
    "    return avg_position_difference, cumulative_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config = {\"action_space\": [-math.pi/18, math.pi/18],\n",
    "          \"observation_space\": [\"x\", \"y\", \"heading_delta\"],\n",
    "          \"heading_delta_reward_coefficient\": 5,\n",
    "          \"goal_distance_reward_coefficient\": 5,\n",
    "          \"avoidance_reward\": 50,\n",
    "          \"game_over_penalty\": 50000,\n",
    "          \"success_reward\": 50000,\n",
    "          \"policy\": \"MlpPolicy\",\n",
    "          \"discount_factor\": 0.99,\n",
    "          \"learning_rate\": 0.00025,\n",
    "          \"entropy_coefficient\": 0.01,\n",
    "          \"value_function_coefficient\": 0.5,\n",
    "          \"n_env\": 1,\n",
    "          \"model\": \"PPO2\",\n",
    "          \"n_obstacles\": 10,\n",
    "          \"velocity\": 5,\n",
    "          \"save_path\": \"./PPO2_10_further_examination/convergence_2\",\n",
    "          \"timesteps\": 1000000\n",
    "         }\n",
    "\n",
    "\n",
    "wandb_logger = wandb.init(project='ppo2_10_further_examination_train', config=config)\n",
    "\n",
    "train_model(config, \"./PPO2_10_further_examination/convergence_2\",\n",
    "            \"./PPO2_10_further_examination/convergence_2.json\",\n",
    "            wandb_logger=wandb_logger, total_timesteps=1000000\n",
    "           )\n",
    "wandb_logger.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Tuning (Randomised Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "heading_change_coefs = [0.1, 0.5, 1, 5, 10]\n",
    "\n",
    "goal_distance_coefs = [0.01, 0.05, 0.1, 0.5, 1]\n",
    "\n",
    "avoidance_rewards = [0.1, 0.5, 1, 5, 10]\n",
    "\n",
    "game_over_penalties = [100, 500, 1000, 5000, 10000]\n",
    "\n",
    "mission_success_rewards = [100, 500, 1000, 5000, 10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Try 100 different parameters\n",
    "for i in range(0, 100):\n",
    "    heading_change_coef = random.sample(heading_change_coefs, 1)[0]\n",
    "    goal_distance_coef = random.sample(goal_distance_coefs, 1)[0]\n",
    "    avoidance_reward = random.sample(avoidance_rewards, 1)[0]\n",
    "    game_over_penalty = random.sample(game_over_penalties, 1)[0]\n",
    "    mission_success_reward = random.sample(mission_success_rewards, 1)[0]\n",
    "\n",
    "    # Train with PPO\n",
    "    config = {\"action_space\": [-math.pi/18, math.pi/18],\n",
    "              \"observation_space\": [\"x\", \"y\", \"heading_delta\"],\n",
    "              \"heading_delta_reward_coefficient\": heading_change_coef,\n",
    "              \"goal_distance_reward_coefficient\": goal_distance_coef,\n",
    "              \"avoidance_reward\": avoidance_reward,\n",
    "              \"game_over_penalty\": game_over_penalty,\n",
    "              \"success_reward\": mission_success_reward,\n",
    "              \"policy\": \"MlpPolicy\",\n",
    "              \"discount_factor\": 0.99,\n",
    "              \"learning_rate\": 0.00025,\n",
    "              \"entropy_coefficient\": 0.01,\n",
    "              \"value_function_coefficient\": 0.5,\n",
    "              \"n_env\": 1,\n",
    "              \"model\": \"PPO2\",\n",
    "             }\n",
    "    \n",
    "    train_model(config, f\"./PPO2_tuning_10_obstacles/{i}\", f\"./PPO2_tuning_10_obstacles/model_{i}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO2 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, 55):\n",
    "    eval_model(f\"./PPO2_tuning_10_obstacles/{i}\", f\"./PPO2_tuning_10_obstacles/model_{i}.json\", eval_wandb_project='PPO2_tuning_10_obstacles_eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EA Classic Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "EA_dir = root_dir.joinpath(\"EA_classic_control\")\n",
    "reward_type_dir = EA_dir.joinpath(\"original_rewards_actions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 0\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\policies.py:561: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:332: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\distributions.py:418: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:194: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:202: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\ops\\math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:210: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:244: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\PREDATOR\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py:246: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
      "\n",
      "model created\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-3707d94ae03f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     train_model(config, model_save_path, model_config_save_path,\n\u001b[1;32m---> 21\u001b[1;33m                 \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MountainCarContinuous-v0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m                )\n\u001b[0;32m     23\u001b[0m     fitness_result, cumulative_reward = eval_model(model_save_path, model_config_save_path, episodes=10, eval_wandb_project=None,\n",
      "\u001b[1;32m<ipython-input-6-19a3f0dda7dc>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(env_kwargs, model_save_path, model_config_save_path, total_timesteps, wandb_logger, model_load_path, model_config_load_path, env_name)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model created\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model has finished learning'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_rollout_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                 \u001b[1;31m# true_reward is the reward without discount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m                 \u001b[0mrollout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m                 \u001b[1;31m# Unpack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\runners.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, callback)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0mep_infos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneglogpacs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m             \u001b[0mmb_obs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[0mmb_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\policies.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, obs, state, mask, deterministic)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n\u001b[1;32m--> 576\u001b[1;33m                                                    {self.obs_ph: obs})\n\u001b[0m\u001b[0;32m    577\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneglogp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train a base population\n",
    "population_save_path = \"./EA_classic_control/original_rewards_distance_10000000_timesteps/population.json\"\n",
    "\n",
    "# If there is no data in the population json\n",
    "population = {}\n",
    "# Load the population data from the json if there is\n",
    "# with open(population_save_path) as f:\n",
    "#     population = json.load(f)\n",
    "\n",
    "timesteps = 10000000\n",
    "\n",
    "for i in range(0, 100):\n",
    "    print(f'model {i}')\n",
    "    config = {'completion_reward_coef': 100,\n",
    "              'action_reward_coef': 0.1\n",
    "             }\n",
    "    model_save_path = f\"./EA_classic_control/original_rewards_distance_10000000_timesteps/model_{i}\"\n",
    "    model_config_save_path = f\"./EA_classic_control/original_rewards_distance_10000000_timesteps/model_{i}_config.json\"\n",
    "\n",
    "    train_model(config, model_save_path, model_config_save_path,\n",
    "                total_timesteps=timesteps, env_name='MountainCarContinuous-v0'\n",
    "               )\n",
    "    fitness_result, cumulative_reward = eval_model(model_save_path, model_config_save_path, episodes=10, eval_wandb_project=None,\n",
    "                                env_name='MountainCarContinuous-v0'\n",
    "                               )\n",
    "    \n",
    "    population[model_save_path] = {'config': config,\n",
    "                                   'fitness': fitness_result,\n",
    "                                   'timesteps': timesteps\n",
    "                                  }\n",
    "    \n",
    "    with open(population_save_path, 'w') as fp:\n",
    "        json.dump(population, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winning model from tournament selection: ('./EA_classic_control/original_rewards_distance/model_61', {'config': {'completion_reward_coef': 100, 'action_reward_coef': 0.1}, 'fitness': 0.9074695318937301, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 141.9803548682828, 'action_reward_coef': 0.09253926029376391}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_237\n",
      "winning model from tournament selection: ('./EA_classic_control/original_rewards_distance/model_84', {'config': {'completion_reward_coef': 100, 'action_reward_coef': 0.1}, 'fitness': 0.9291738599538804, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 89.48886843160098, 'action_reward_coef': 0.1005451548644256}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_522\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_434', {'config': {'completion_reward_coef': 87.01124243592301, 'action_reward_coef': 0.08178382982842987}, 'fitness': 0.9144164532423019, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 73.55895312981636, 'action_reward_coef': 0.07585071836203372}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_523\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_426', {'config': {'completion_reward_coef': 79.44181509467207, 'action_reward_coef': 0.06305918359547061}, 'fitness': 0.9110030174255371, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 80.35884533276878, 'action_reward_coef': 0.04560851287715793}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_524\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_226', {'config': {'completion_reward_coef': 129.86556329986846, 'action_reward_coef': 0.08889310691499011}, 'fitness': 0.9325998365879059, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 118.0118394269096, 'action_reward_coef': 0.10799017172277213}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_525\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_132', {'config': {'completion_reward_coef': 157.66603753213658, 'action_reward_coef': 0.1056643143100568}, 'fitness': 0.9193200439214706, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 221.55168127192374, 'action_reward_coef': 0.1051922449536989}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_526\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_203', {'config': {'completion_reward_coef': 70.8777804805284, 'action_reward_coef': 0.19407216664214944}, 'fitness': 0.9169093877077102, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 70.03373978599788, 'action_reward_coef': 0.18917990445769595}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_527\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_513', {'config': {'completion_reward_coef': 332.2838333194628, 'action_reward_coef': 0.02318008266461428}, 'fitness': 0.9209483832120895, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 435.66775165463116, 'action_reward_coef': 0.02017564644893217}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_528\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_515', {'config': {'completion_reward_coef': 96.48713393383925, 'action_reward_coef': 0.07952641838848695}, 'fitness': 0.9150989651679993, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 145.37481452217105, 'action_reward_coef': 0.11563500983840086}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_529\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_154', {'config': {'completion_reward_coef': 52.55781003096336, 'action_reward_coef': 0.06032565364257935}, 'fitness': 0.9272885024547577, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 50.61346311970337, 'action_reward_coef': 0.05733743228891015}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_530\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_154', {'config': {'completion_reward_coef': 52.55781003096336, 'action_reward_coef': 0.06032565364257935}, 'fitness': 0.9272885024547577, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 50.37199343467273, 'action_reward_coef': 0.05312230082308677}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_531\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_328', {'config': {'completion_reward_coef': 270.56479314149396, 'action_reward_coef': 0.023318280217444476}, 'fitness': 0.9118266373872757, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 168.67772408416874, 'action_reward_coef': 0.01830460414992034}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_492\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_261', {'config': {'completion_reward_coef': 46.12964847455068, 'action_reward_coef': 0.22941946589865456}, 'fitness': 0.9248531132936477, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 50.05176162058372, 'action_reward_coef': 0.15292991879706513}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_533\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_420', {'config': {'completion_reward_coef': 97.38269293345263, 'action_reward_coef': 0.09250757049404106}, 'fitness': 0.9009832322597504, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 85.47785404198818, 'action_reward_coef': 0.11708748156266413}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_534\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_213', {'config': {'completion_reward_coef': 129.0476436264165, 'action_reward_coef': 0.11482730262188888}, 'fitness': 0.9052988409996032, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 153.7336414803257, 'action_reward_coef': 0.10065619318803642}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_535\n",
      "winning model from tournament selection: ('./EA_classic_control/original_rewards_distance/model_14', {'config': {'completion_reward_coef': 100, 'action_reward_coef': 0.1}, 'fitness': 0.927482956647873, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 93.04530779960693, 'action_reward_coef': 0.0970139945092544}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_536\n",
      "winning model from tournament selection: ('./EA_classic_control/original_rewards_distance/model_25', {'config': {'completion_reward_coef': 100, 'action_reward_coef': 0.1}, 'fitness': 0.9147607415914536, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 101.19136841153117, 'action_reward_coef': 0.10627048870828848}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_537\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_320', {'config': {'completion_reward_coef': 131.0063038452691, 'action_reward_coef': 0.09660176340064514}, 'fitness': 0.9240843385457993, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 101.00207430386264, 'action_reward_coef': 0.10421541101792248}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_538\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_317', {'config': {'completion_reward_coef': 78.83363349037286, 'action_reward_coef': 0.10951063373218031}, 'fitness': 0.9174039036035537, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 96.68109654329746, 'action_reward_coef': 0.06780717401086608}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_539\n",
      "winning model from tournament selection: ('./EA_classic_control/original_rewards_distance/model_61', {'config': {'completion_reward_coef': 100, 'action_reward_coef': 0.1}, 'fitness': 0.9074695318937301, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 131.425590935179, 'action_reward_coef': 0.10718875994729368}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_417\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_284', {'config': {'completion_reward_coef': 135.9455088875181, 'action_reward_coef': 0.03879630572557692}, 'fitness': 0.9174787312746048, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 117.82103653028089, 'action_reward_coef': 0.04395302369705272}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_541\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_419', {'config': {'completion_reward_coef': 141.09065987440943, 'action_reward_coef': 0.11749845150202866}, 'fitness': 0.9216378837823868, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 175.59945635856454, 'action_reward_coef': 0.16383561429873653}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_508\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_143', {'config': {'completion_reward_coef': 86.43955605419264, 'action_reward_coef': 0.08967505066234546}, 'fitness': 0.9240725874900818, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 82.24089528312155, 'action_reward_coef': 0.12398502380866096}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_498\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_132', {'config': {'completion_reward_coef': 157.66603753213658, 'action_reward_coef': 0.1056643143100568}, 'fitness': 0.9193200439214706, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 146.3723608424702, 'action_reward_coef': 0.0578250266387843}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_270\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_375', {'config': {'completion_reward_coef': 114.13200280419954, 'action_reward_coef': 0.08224977431094718}, 'fitness': 0.9259634286165237, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 78.15966289509652, 'action_reward_coef': 0.06224764982970617}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_545\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_426', {'config': {'completion_reward_coef': 79.44181509467207, 'action_reward_coef': 0.06305918359547061}, 'fitness': 0.9110030174255371, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 91.18796519159834, 'action_reward_coef': 0.061137170252504196}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_546\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_203', {'config': {'completion_reward_coef': 70.8777804805284, 'action_reward_coef': 0.19407216664214944}, 'fitness': 0.9169093877077102, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 115.31667793024218, 'action_reward_coef': 0.188314331658914}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_547\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_420', {'config': {'completion_reward_coef': 97.38269293345263, 'action_reward_coef': 0.09250757049404106}, 'fitness': 0.9009832322597504, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 101.57394610815298, 'action_reward_coef': 0.12002366771214107}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_543\n",
      "winning model from tournament selection: ('./EA_classic_control/original_rewards_distance/model_73', {'config': {'completion_reward_coef': 100, 'action_reward_coef': 0.1}, 'fitness': 0.9273641169071197, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 89.08519135803833, 'action_reward_coef': 0.07595190551583833}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_549\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_203', {'config': {'completion_reward_coef': 70.8777804805284, 'action_reward_coef': 0.19407216664214944}, 'fitness': 0.9169093877077102, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 44.55422175313958, 'action_reward_coef': 0.22474894396310613}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_550\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_284', {'config': {'completion_reward_coef': 135.9455088875181, 'action_reward_coef': 0.03879630572557692}, 'fitness': 0.9174787312746048, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 121.37280453056349, 'action_reward_coef': 0.05195005350959559}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_551\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_216', {'config': {'completion_reward_coef': 115.05099712743491, 'action_reward_coef': 0.08609048490642919}, 'fitness': 0.922452986240387, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 91.1171867472874, 'action_reward_coef': 0.04638253991022421}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_484\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_120', {'config': {'completion_reward_coef': 119.27647214077875, 'action_reward_coef': 0.11435046104515526}, 'fitness': 0.9144970566034317, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 137.68098495585082, 'action_reward_coef': 0.12393661551614776}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_553\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_418', {'config': {'completion_reward_coef': 76.465038868861, 'action_reward_coef': 0.10561114613295879}, 'fitness': 0.8952282071113586, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 69.67136588582608, 'action_reward_coef': 0.1474254837951267}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_554\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_519', {'config': {'completion_reward_coef': 80.31285692240236, 'action_reward_coef': 0.10521728825789914}, 'fitness': 0.9143309980630875, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 85.77217817236716, 'action_reward_coef': 0.10304179316426737}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_555\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_305', {'config': {'completion_reward_coef': 107.68093490561903, 'action_reward_coef': 0.12788880580842707}, 'fitness': 0.9160427004098892, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 96.23398101777904, 'action_reward_coef': 0.07289924582345728}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_556\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_540', {'config': {'completion_reward_coef': 131.425590935179, 'action_reward_coef': 0.10718875994729368}, 'fitness': 0.9175760447978973, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 155.1844711888837, 'action_reward_coef': 0.0919266844104289}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_557\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_367', {'config': {'completion_reward_coef': 59.78525781404487, 'action_reward_coef': 0.09695204649643785}, 'fitness': 0.9340803176164627, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 74.20902787771305, 'action_reward_coef': 0.11190784252786838}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_558\n",
      "winning model from tournament selection: ('./EA_classic_control/original_rewards_distance/model_19', {'config': {'completion_reward_coef': 100, 'action_reward_coef': 0.1}, 'fitness': 0.9213748127222061, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 98.4228605598205, 'action_reward_coef': 0.05075128861871317}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_559\n",
      "winning model from tournament selection: ('./EA_classic_control/original_rewards_distance/model_7', {'config': {'completion_reward_coef': 100, 'action_reward_coef': 0.1}, 'fitness': 0.9097050845623016, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 117.3358042694684, 'action_reward_coef': 0.10195785637817814}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_560\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_430', {'config': {'completion_reward_coef': 74.98686695963502, 'action_reward_coef': 0.11007613851725989}, 'fitness': 0.9227598279714584, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 32.50321779331636, 'action_reward_coef': 0.13196040132356399}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_561\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_354', {'config': {'completion_reward_coef': 89.12391132430149, 'action_reward_coef': 0.08830694760962128}, 'fitness': 0.9302485287189484, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 69.59732918177977, 'action_reward_coef': 0.10733950042681512}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_562\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_123', {'config': {'completion_reward_coef': 67.38751717764052, 'action_reward_coef': 0.14978778518067487}, 'fitness': 0.9107021003961563, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 50.5249448445432, 'action_reward_coef': 0.13993255157319479}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_179\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_317', {'config': {'completion_reward_coef': 78.83363349037286, 'action_reward_coef': 0.10951063373218031}, 'fitness': 0.9174039036035537, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 82.87296709867763, 'action_reward_coef': 0.12142375068039726}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_564\n",
      "winning model from tournament selection: ('./EA_classic_control/original_rewards_distance/model_16', {'config': {'completion_reward_coef': 100, 'action_reward_coef': 0.1}, 'fitness': 0.9042024075984955, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 109.26194275362856, 'action_reward_coef': 0.08050268306521902}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_565\n",
      "winning model from tournament selection: ('./EA_classic_control/original_rewards_distance/model_25', {'config': {'completion_reward_coef': 100, 'action_reward_coef': 0.1}, 'fitness': 0.9147607415914536, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 65.4222427411269, 'action_reward_coef': 0.06361845860726517}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_566\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_350', {'config': {'completion_reward_coef': 106.79017233746714, 'action_reward_coef': 0.1052737380317753}, 'fitness': 0.9159703433513642, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 113.44400409201236, 'action_reward_coef': 0.1181304977956727}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_567\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_405', {'config': {'completion_reward_coef': 93.33273629987178, 'action_reward_coef': 0.08529060311051986}, 'fitness': 0.9301902711391449, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 72.75327553032388, 'action_reward_coef': 0.0783748970770289}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_568\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_418', {'config': {'completion_reward_coef': 76.465038868861, 'action_reward_coef': 0.10561114613295879}, 'fitness': 0.8952282071113586, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 98.92081893796828, 'action_reward_coef': 0.10823276264855156}\n",
      "model created\n",
      "Model has finished learning\n",
      "Model successfully saved\n",
      "episode: 0\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 1\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 2\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 3\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 4\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 5\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 6\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 7\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 8\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "episode: 9\n",
      "episode action count: 999\n",
      "Agent failed episode through timeout\n",
      "evaluation complete\n",
      "weakest model: ./EA_classic_control/EA_rewards_distance/model_569\n",
      "winning model from tournament selection: ('./EA_classic_control/EA_rewards_distance/model_320', {'config': {'completion_reward_coef': 131.0063038452691, 'action_reward_coef': 0.09660176340064514}, 'fitness': 0.9240843385457993, 'timesteps': 100000})\n",
      "evolved_model rewards: {'completion_reward_coef': 111.04674854080629, 'action_reward_coef': 0.11187239145382064}\n",
      "model created\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-fe1b0e6caa4f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     train_model(winner_rewards, evolved_model_path, evolved_model_config_path,\n\u001b[1;32m---> 42\u001b[1;33m                 \u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_timesteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'MountainCarContinuous-v0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m                )\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-19a3f0dda7dc>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(env_kwargs, model_save_path, model_config_save_path, total_timesteps, wandb_logger, model_load_path, model_config_load_path, env_name)\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"model created\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model has finished learning'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mcallback\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_rollout_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m                 \u001b[1;31m# true_reward is the reward without discount\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 343\u001b[1;33m                 \u001b[0mrollout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    344\u001b[0m                 \u001b[1;31m# Unpack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneglogpacs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mep_infos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrue_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrollout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\runners.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, callback)\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontinue_training\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\ppo2\\ppo2.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    478\u001b[0m         \u001b[0mep_infos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    479\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 480\u001b[1;33m             \u001b[0mactions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneglogpacs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    481\u001b[0m             \u001b[0mmb_obs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m             \u001b[0mmb_actions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\stable_baselines\\common\\policies.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, obs, state, mask, deterministic)\u001b[0m\n\u001b[0;32m    574\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    575\u001b[0m             action, value, neglogp = self.sess.run([self.action, self.value_flat, self.neglogp],\n\u001b[1;32m--> 576\u001b[1;33m                                                    {self.obs_ph: obs})\n\u001b[0m\u001b[0;32m    577\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneglogp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    954\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 956\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    957\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1180\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1181\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1357\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1359\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1363\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1365\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1366\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[1;32m-> 1350\u001b[1;33m                                       target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\hilla_baselines\\lib\\site-packages\\tensorflow_core\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1443\u001b[1;33m                                             run_metadata)\n\u001b[0m\u001b[0;32m   1444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1445\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# EA\n",
    "max_iterations = 10000\n",
    "population_size = 521\n",
    "training_timesteps = 100000\n",
    "\n",
    "# Load the population\n",
    "population_save_path = f'./EA_classic_control/EA_rewards_distance/population.json'\n",
    "with open(population_save_path) as f:\n",
    "    population = json.load(f)\n",
    "\n",
    "for i in range(max_iterations):\n",
    "    # Tournament Selection\n",
    "    model1, model2 = random.sample(population.items(), 2)\n",
    "    model1_fitness = model1[1]['fitness']\n",
    "    model2_fitness = model2[1]['fitness']\n",
    "    \n",
    "    # Compare fitness\n",
    "    # For the continuous mountain car env, lower time is greater fitness\n",
    "    if model1_fitness > model2_fitness:\n",
    "        winner_model = model2\n",
    "        \n",
    "    else:\n",
    "        winner_model = model1\n",
    "        \n",
    "    print(f'winning model from tournament selection: {winner_model}')\n",
    "        \n",
    "    # Evolve the winning sample\n",
    "    winner_rewards = winner_model[1]['config']\n",
    "    evolved_model_rewards = {}\n",
    "    for key, value in winner_rewards.items():\n",
    "        # Evolve the reward using a gaussian distribution where the current reward is the mean\n",
    "        # A quarter of the current reward is the std\n",
    "        evolved_model_rewards[key] = np.random.normal(value, value/4, 1)[0]\n",
    "        \n",
    "    print(f'evolved_model rewards: {evolved_model_rewards}')\n",
    "        \n",
    "    # Train the evolved model\n",
    "    evolved_model_path = f'./EA_classic_control/EA_rewards_distance/model_{population_size + i}'\n",
    "    evolved_model_config_path = f'./EA_classic_control/EA_rewards_distance/model_{population_size + i}.json'\n",
    "    \n",
    "    train_model(winner_rewards, evolved_model_path, evolved_model_config_path,\n",
    "                total_timesteps=training_timesteps, env_name='MountainCarContinuous-v0'\n",
    "               )\n",
    "    \n",
    "    # Evaluate evolved model's fitness\n",
    "    fitness_result, reward = eval_model(evolved_model_path, evolved_model_config_path, episodes=10, eval_wandb_project=None,\n",
    "                                        env_name='MountainCarContinuous-v0'\n",
    "                                       )\n",
    "    \n",
    "    # Add the evolved model and its details into the population\n",
    "    population[evolved_model_path] = {'config': evolved_model_rewards,\n",
    "                                      'fitness': fitness_result,\n",
    "                                      'timesteps': training_timesteps\n",
    "                                     }\n",
    "    \n",
    "    # Delete the worst individual in the population\n",
    "    weakest_model_path = ''\n",
    "    weakest_model_fitness = -math.inf\n",
    "    for model_path, model_details in population.items():\n",
    "        if model_details['fitness'] > weakest_model_fitness:\n",
    "            weakest_model_fitness = model_details['fitness']\n",
    "            weakest_model_path = model_path\n",
    "            \n",
    "    print(f'weakest model: {weakest_model_path}')\n",
    "            \n",
    "    del population[weakest_model_path]\n",
    "    \n",
    "    with open(population_save_path, 'w') as fp:\n",
    "        json.dump(population, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_population(population_save_path):\n",
    "    # Load the population\n",
    "    with open(population_save_path) as f:\n",
    "        population = json.load(f)\n",
    "\n",
    "    best_fitness = math.inf\n",
    "    best_model = 'NIL'\n",
    "    fitness_values = []\n",
    "    for model_path, model_details in population.items():\n",
    "        fitness_values.append(model_details['fitness'])\n",
    "        if model_details['fitness'] < best_fitness:\n",
    "            best_model = model_path\n",
    "            best_fitness = model_details['fitness']\n",
    "\n",
    "    plt.hist(fitness_values, bins=100)\n",
    "    plt.title(\"Distribution of population fitness\")\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model, best_fitness, fitness_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model, best_fitness, no_rewards_fitness = evaluate_population(\"./EA_classic_control/no_rewards_distance/population.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXEAAAEICAYAAACpqsStAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAATtUlEQVR4nO3deZRkZXnH8e/DzAAqu7SErWmNS0TPEXHEuKAEJYKDcqIkYMQDCJkQ94UYEDhHPBpIVKIEc3SChIALYhSjjkRQGXFhERQQGBAYB9lkZBMGN5Ynf9x3hqKcnrrVXdXV7/D9nFOnb9W9963nvbfq17fee6s7MhNJUp3WG3UBkqSpM8QlqWKGuCRVzBCXpIoZ4pJUMUNckipmiM8iEfHJiDhmQG2NR8TKiJhT7i+JiEMH0XZp7+yIOHBQ7fXxvB+MiDsi4pcz/dyTiYiJiMiImDvF9d8XEScPuq7S9urt1f2a0LohvE58ZkTEcmAr4EHgIeBq4DRgUWY+PIW2Ds3Mb/WxzhLgM5nZd1hExPuBp2bmAf2uO0gRMQ5cC+yQmStGWUuniJgAfg7My8wHeyy7G81+2G4G6lrr9prOa0Kzh0fiM+vVmbkxsANwPPBPwKcH/SRTPSKswDhw52wK8FnO7fVYkJneZuAGLAde0fXYLsDDwLPL/VOBD5bpLYGvA/cAdwHfo/mle3pZ57fASuC9wASQwCHAL4DzOx6bW9pbAhwHXAzcC/wvsEWZtxtw85rqBfYE/gA8UJ7v8o72Di3T6wFHAzcCK2g+YWxa5q2q48BS2x3AUWvZTpuW9X9V2ju6tP+K0ueHSx2nrmHd3YCbgfeV51kOvKFX22XeQcAPgJOAXwPXAC+fbP8B76c5iu3s46ptfTCwFLgPWAb8fXn8CV19WAls09lWWe41wFVl3y8BntlVx+HAFaXOLwAbrmFb/NH26qwT+BDNJ8LflfknlfUSOAy4rjz/Jyif2Mv8N5W+3Q18k+YoHyCAfyv7/17gpzzyun4VzSfP+4BbgMNH/X5cl24jL+CxcusOgY7HfwH8Q5k+lUdC/Djgk8C8ctuVR4a/ugNl1ZvztBIUj1tDsCwpb6Bnl2W+1BFCuzFJiJfpR4VMR3urQvxNwPXAU4CNgC8Dp3fV9p+lrucAv+8Mpq52T6P5BbNxWfdnwCGT1dm17m40w1UnABsALwPuB57Rou2DyrrvKtt7P5qQ3GKSbb56m6xhWy8A/pQm2F4G/AbYeS3burOtp5ea9yh1vLds2/U76riYJvy3oAnUw9ayPW7uuN9d5+p92LFM0hw8bEZzJP8rYM8yb59SyzNpfhEcDfywzHslcGlZL8oyW5d5twG7lunNV20Lb4O5OZwyerfSvBm7PQBsTXOk80Bmfi/Lu2At3p+Z92fmbyeZf3pmXpmZ9wPHAH8zoJNcbwBOyMxlmbkSOBLYv2tY59jM/G1mXg5cThPmj1Jq2R84MjPvy8zlwEeBN/ZZzzGZ+fvM/C6wmEf62avtFcDHyvb+As148oI+n5vMXJyZN2Tju8A5NL+E29gPWJyZ52bmA8BHaH75vahjmRMz89bMvAv4GrBTvzX2cHxm3pOZvwDO62j/MOC4zFyazdj/PwM7RcQONK/XjYE/oznYWJqZt5X1HgB2jIhNMvPuzPzxgOt9TDPER29bmuGSbh+mOeo5JyKWRcQRLdq6qY/5N9Ic6W3Zqsq126a019n2XJoTuat0Xk3yG5oj9m5blpq629q2j1ruLr+kOtffpmXbt3T9oly1bl8iYq+IuDAi7oqIe2iGE9pu50dty2xOet/UVWebbTkdk7W/A/DxiLin9OsumqPubTPzOzRDUZ8AVkTEoojYpKz3OpptcGNEfDciXjjgeh/TDPERiojn07w5v989rxwtviczn0IzRvruiHj5qtmTNNnrSH37julxmiOkO2g+vj++o645wFgf7d5K8wbvbPtB4PYe63W7o9TU3dYtfbSxeUQ8oWv9W1u2vW1ExBrWha5tBPzJmp48IjagGar6CLBVZm4GfIMm7KDPbVnq2Z7+tkFb/V6adhPN+P5mHbfHZeYPATLzxMx8HrAjzbDQP5bHf5SZ+wBPAr4CnDmwHsgQH4WI2CQi9gbOoBkL/ekaltk7Ip5a3sS/pjkJtepSxNtpxp/7dUBE7BgRjwc+APxPZj5EMza8YUQsiIh5NGOdG3SsdzswERGTvV4+D7wrIp4cERvRfMz+Qva43K5bqeVM4EMRsXH5mP5u4DP9tAMcGxHrR8SuwN7AF1u2/STg7RExLyL+mmZc9xtl3mU0Q0TzImI+sO8kz70+zbb7FfBgROwF/GXH/NuBJ0bEppOsfyawICJeXvbFe2jOIfywnw3QUr+vo08CR0bEswAiYtOynYiI50fEC0rN99OcMH247Ic3RMSmZXjoXh55HWsADPGZ9bWIuI/miOYomhNwB0+y7NOAb9FcOXAB8B+ZeV6ZdxxwdPlYe3gfz386zcnTXwIbAm8HyMxfA28GTqY54ruf5iqPVb5Yft4ZEWsazzyltH0+zfXSvwPe1kddnd5Wnn8ZzSeUz5X22/olzZUTtwKfpTnpd03Lti+i2e530Fy9sW9m3lnmHUNzsvJu4Niy7h/JzPtotuuZZdm/Bb7aMf8aml96y8r+26Zr/WuBA4B/L3W8mubS1D/0sQ3a+jiwb0TcHREn9lo4M88C/gU4IyLuBa4E9iqzN6E5eX03zXDQnTRDgtCcd1he1jmM5hyKBsQv+2idMZ0v0kTEQTRXarxkwGVJQ+WRuCRVzBCXpIo5nCJJFfNIXJIqNpQ/lLTlllvmxMTEMJqWpHXSpZdeekdmjvVe8tGGEuITExNccsklw2haktZJEXFj76X+mMMpklQxQ1ySKmaIS1LFDHFJqpghLkkVM8QlqWKtLjEs/139Ppo/h/pgZs4fZlGSpHb6uU78LzLzjqFVIknqm8MpklSxtkfiSfO/HhP4VGYu6l4gIhYCCwHGx8cHV6G0FhNHLF49vfz4vv+nsVS9tkfiL8nMnWn+i8dbIuKl3Qtk5qLMnJ+Z88fG+v76vyRpClqFeGbeUn6uAM4CdhlmUZKkdnqGeEQ8ISI2XjVN809frxx2YZKk3tqMiW8FnNX803XmAp/LzP8balWSpFZ6hnhmLgOeMwO1SJL65CWGklQxQ1ySKmaIS1LFDHFJqpghLkkVM8QlqWKGuCRVzBCXpIoZ4pJUMUNckipmiEtSxQxxSaqYIS5JFTPEJalihrgkVcwQl6SKGeKSVDFDXJIqZohLUsUMcUmqmCEuSRUzxCWpYoa4JFXMEJekihniklQxQ1ySKmaIS1LFDHFJqpghLkkVM8QlqWKGuCRVzBCXpIq1DvGImBMRP4mIrw+zIElSe/0cib8DWDqsQiRJ/WsV4hGxHbAAOHm45UiS+jG35XIfA94LbDzZAhGxEFgIMD4+Pu3CpNlg4ojFq6eXH79gyut2Wls703k+PTb1PBKPiL2BFZl56dqWy8xFmTk/M+ePjY0NrEBJ0uTaDKe8GHhNRCwHzgB2j4jPDLUqSVIrPUM8M4/MzO0ycwLYH/hOZh4w9MokST15nbgkVaztiU0AMnMJsGQolUiS+uaRuCRVzBCXpIoZ4pJUMUNckipmiEtSxQxxSaqYIS5JFTPEJalihrgkVcwQl6SKGeKSVDFDXJIqZohLUsUMcUmqmCEuSRUzxCWpYoa4JFXMEJekihniklQxQ1ySKmaIS1LFDHFJqpghLkkVM8QlqWKGuCRVzBCXpIoZ4pJUMUNckipmiEtSxQxxSaqYIS5JFTPEJaliPUM8IjaMiIsj4vKIuCoijp2JwiRJvc1tsczvgd0zc2VEzAO+HxFnZ+aFQ65NktRDzxDPzARWlrvzyi2HWZQkqZ1WY+IRMSciLgNWAOdm5kVDrUqS1Eqb4RQy8yFgp4jYDDgrIp6dmVd2LhMRC4GFAOPj44OuU5WYOGLx6unlxy+YNW1N9Xn7XX4m6pzs+Sare9g1talnJvffY01fV6dk5j3AecCea5i3KDPnZ+b8sbGxAZUnSVqbNlenjJUjcCLiccAewDVDrkuS1EKb4ZStgf+OiDk0oX9mZn59uGVJktpoc3XKFcBzZ6AWSVKf/MamJFXMEJekihniklQxQ1ySKmaIS1LFDHFJqpghLkkVM8QlqWKGuCRVzBCXpIoZ4pJUMUNckipmiEtSxQxxSaqYIS5JFTPEJalihrgkVcwQl6SKGeKSVDFDXJIqZohLUsUMcUmqmCEuSRUzxCWpYoa4JFXMEJekihniklQxQ1ySKmaIS1LFDHFJqpghLkkVM8QlqWKGuCRVrGeIR8T2EXFeRFwdEVdFxDtmojBJUm9zWyzzIPCezPxxRGwMXBoR52bm1UOuTZLUQ88j8cy8LTN/XKbvA5YC2w67MElSb22OxFeLiAngucBFa5i3EFgIMD4+PojaqjRxxOJH3V9+/IIRVTJc3f3stczatkPb5QZtGM/bZrvMlnYn6/+w98dUXhed1tX31FS1PrEZERsBXwLemZn3ds/PzEWZOT8z54+NjQ2yRknSJFqFeETMownwz2bml4dbkiSprTZXpwTwaWBpZp4w/JIkSW21ORJ/MfBGYPeIuKzcXjXkuiRJLfQ8sZmZ3wdiBmqRJPXJb2xKUsUMcUmqmCEuSRUzxCWpYoa4JFXMEJekihniklQxQ1ySKmaIS1LFDHFJqpghLkkVM8QlqWKGuCRVzBCXpIoZ4pJUMUNckipmiEtSxQxxSaqYIS5JFTPEJalihrgkVcwQl6SKGeKSVDFDXJIqZohLUsUMcUmqmCEuSRUzxCWpYoa4JFXMEJekihniklQxQ1ySKtYzxCPilIhYERFXzkRBkqT22hyJnwrsOeQ6JElT0DPEM/N84K4ZqEWS1Ke5g2ooIhYCCwHGx8en3M7EEYtXTy8/fsG065pNOvvWqbOf0+l/m/b7XbffdqbT/tqW63e7TGdb9NvmdEy3zWHUNFn7w3g/TqX9Ye/bydqZbJlRZ9bATmxm5qLMnJ+Z88fGxgbVrCRpLbw6RZIqZohLUsXaXGL4eeAC4BkRcXNEHDL8siRJbfQ8sZmZr5+JQiRJ/XM4RZIqZohLUsUMcUmqmCEuSRUzxCWpYoa4JFXMEJekihniklQxQ1ySKmaIS1LFDHFJqpghLkkVM8QlqWKGuCRVzBCXpIoZ4pJUMUNckipmiEtSxQxxSaqYIS5JFTPEJalihrgkVcwQl6SKGeKSVDFDXJIqZohLUsUMcUmqmCEuSRUzxCWpYoa4JFXMEJekihniklQxQ1ySKtYqxCNiz4i4NiKuj4gjhl2UJKmdniEeEXOATwB7ATsCr4+IHYddmCSptzZH4rsA12fmssz8A3AGsM9wy5IktRGZufYFIvYF9szMQ8v9NwIvyMy3di23EFhY7j4DuHbw5Y7clsAdoy5iSOxbvdbl/j2W+rZDZo7128jcQVWTmYuARYNqbzaKiEsyc/6o6xgG+1avdbl/9q23NsMptwDbd9zfrjwmSRqxNiH+I+BpEfHkiFgf2B/46nDLkiS10XM4JTMfjIi3At8E5gCnZOZVQ69sdlqXh4vsW73W5f7Ztx56ntiUJM1efmNTkipmiEtSxQxxev9ZgYjYISK+HRFXRMSSiNiuY96BEXFduR04s5X3NtW+RcROEXFBRFxV5u0389X3Np19V+ZvEhE3R8RJM1d1O9N8XY5HxDkRsTQiro6IiRktvoVp9u9fy2tzaUScGBExs9VPLiJOiYgVEXHlJPOj1Hx96dvOHfP6z5PMfEzfaE7W3gA8BVgfuBzYsWuZLwIHlundgdPL9BbAsvJz8zK9+aj7NKC+PR14WpneBrgN2GzUfRpU/zrmfxz4HHDSqPszyL4BS4A9yvRGwONH3acBvjZfBPygtDEHuADYbdR96qj7pcDOwJWTzH8VcDYQwJ8DF5XHp5QnHom3+7MCOwLfKdPndcx/JXBuZt6VmXcD5wJ7zkDNbU25b5n5s8y8rkzfCqwA+v422ZBNZ98REc8DtgLOmYFa+zXlvpW/bTQ3M88FyMyVmfmbmSm7tensuwQ2pAn/DYB5wO1Dr7ilzDwfuGsti+wDnJaNC4HNImJrppgnhjhsC9zUcf/m8liny4HXlum/AjaOiCe2XHeUptO31SJiF5o3zA1DqnOqpty/iFgP+Chw+NCrnJrp7LunA/dExJcj4icR8eHyh+xmkyn3LzMvoAn128rtm5m5dMj1DtJkfZ9Snhji7RwOvCwifgK8jOYbqw+NtqSBWWvfyhHC6cDBmfnwaEqclsn692bgG5l58yiLm6bJ+jYX2LXMfz7NkMVBI6pxOtbYv4h4KvBMmm+PbwvsHhG7jq7M0RrY306pWM8/K1CGE14LEBEbAa/LzHsi4hZgt651lwyz2D5NuW/l/ibAYuCo8rFvtpnOvnshsGtEvJlmzHj9iFiZmbPl7+VPp283A5dl5rIy7ys0Y6+fnoG625pO//4OuDAzV5Z5ZwMvBL43E4UPwGR9n1qejPokwKhvNL/IlgFP5pETLM/qWmZLYL0y/SHgAx0nIn5OcxJi8zK9xaj7NKC+rQ98G3jnqPsxjP51LXMQs+/E5nT23Zyy/Fi5/1/AW0bdpwH2bz/gW6WNeeV1+upR96mr9gkmP7G5gEef2Ly4PD6lPBl5Z2fDjeZs8c9oxnyPKo99AHhNmd4XuK4sczKwQce6bwKuL7eDR92XQfUNOAB4ALis47bTqPszyH3X0casC/EBvC73AK4AfgqcCqw/6v4M8LU5B/gUsBS4Gjhh1H3p6tfnacbqH6AZ1z4EOAw4rMwPmn+0c0PZP/M71u07T/zavSRVzBObklQxQ1ySKmaIS1LFDHFJqpghLkkVM8QlqWKGuCRV7P8Bp/RDlmFubcEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_model, best_fitness, original_rewards_fitness = evaluate_population(\"./EA_classic_control/original_rewards_distance/population.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = 35\n",
    "plt.hist(no_rewards_fitness, bins, alpha=0.75, label='No Rewards')\n",
    "plt.hist(original_rewards_fitness, bins, alpha=0.75, label='Original Rewards')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hilla_baselines",
   "language": "python",
   "name": "hilla_baselines"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
